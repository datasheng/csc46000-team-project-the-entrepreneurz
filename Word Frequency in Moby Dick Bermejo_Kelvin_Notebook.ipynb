{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1309988-b429-4fb0-8c4c-193582dbec93",
   "metadata": {},
   "source": [
    "![mobydick](mobydick.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e416c-70e7-478a-a3c8-e54f3fdb4a7f",
   "metadata": {},
   "source": [
    "In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n",
    "\n",
    "The Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669862e8",
   "metadata": {},
   "source": [
    "# üìò Moby-Dick Text Mining & Word Frequency Analysis\n",
    "\n",
    "### Summary\n",
    "In this notebook, we scrape the novel Moby-Dick from Project Gutenberg using the requests library, then extract and clean the text with BeautifulSoup. Once the raw text is processed, we perform natural language processing using NLTK to identify the most frequently used words in the novel.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Project Pipeline\n",
    "1. Download the HTML version of _Moby-Dick_ from Project Gutenberg  \n",
    "2. Parse and extract text using BeautifulSoup \n",
    "3. Clean and tokenize the text using NLTK  \n",
    "4. Remove English stopwords  \n",
    "5. Count word frequencies using Counter variable\n",
    "6. Visualize the most common words\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Libraries Used\n",
    "| Purpose | Library |\n",
    "|--------|---------|\n",
    "| Web scraping | `requests`, `BeautifulSoup` |\n",
    "| NLP / tokenization | `nltk` |\n",
    "| Frequency counting | `collections.Counter` |\n",
    "| Visualization (optional) | `matplotlib` or `seaborn` |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Final Goal\n",
    "Produce a ranked list (and optional visualization) of the most frequent words in _Moby-Dick_ after stopword removal.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 85,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1764199111797,
    "lastExecutedByKernel": "90e0e4cf-879e-4da8-87db-f72e927b3fd0",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Request and encode the text\nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\nr = requests.get(url)\nr.encoding = \"utf-8\"\nhtml = r.text",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import and download packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Request and encode the text\n",
    "url = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"utf-8\"\n",
    "html = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e4561-dfb1-4448-87ff-2e5b2ac85ab9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 453,
    "lastExecutedAt": 1764199112251,
    "lastExecutedByKernel": "90e0e4cf-879e-4da8-87db-f72e927b3fd0",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Extracting text from the HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\nmoby_text = soup.get_text()"
   },
   "outputs": [],
   "source": [
    "#Extracting text from the HTML\n",
    "html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "moby_text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1799d-57ba-453f-b383-459b68669f79",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 95,
    "lastExecutedAt": 1764199112348,
    "lastExecutedByKernel": "90e0e4cf-879e-4da8-87db-f72e927b3fd0",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Tokenize Text\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(moby_text)"
   },
   "outputs": [],
   "source": [
    "#Tokenize Text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(moby_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3240f7-1223-4d33-b284-b3ecec87b946",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 46,
    "lastExecutedAt": 1764199112396,
    "lastExecutedByKernel": "90e0e4cf-879e-4da8-87db-f72e927b3fd0",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Converting to lowercase\nwords = [word.lower() for word in tokens]"
   },
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "words = [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f0033-f553-4736-8c6e-4ee6574e1a33",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1764199112445,
    "lastExecutedByKernel": "90e0e4cf-879e-4da8-87db-f72e927b3fd0",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Load in stopwords\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words(\"english\")\nprint(stop_words[:8])",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all']\n"
     ]
    }
   ],
   "source": [
    "#Load in stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71770c64-d5b7-4076-b1dd-747bfebde3cd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 452,
    "lastExecutedAt": 1764199112897,
    "lastExecutedByKernel": "90e0e4cf-879e-4da8-87db-f72e927b3fd0",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Removing stopwords\n# Remove stopwords from the text\nwords_no_stop = [word for word in words if word not in stop_words]"
   },
   "outputs": [],
   "source": [
    "#Removing stopwords\n",
    "# Remove stopwords from the text\n",
    "words_no_stop = [word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58104205-99bf-4f5e-ac46-e9c5b0130be9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 73,
    "lastExecutedAt": 1764199112971,
    "lastExecutedByKernel": "90e0e4cf-879e-4da8-87db-f72e927b3fd0",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Count freq and display top ten\ncount = Counter(words_no_stop)\n\n#Top 10 common words\ntop_ten = count.most_common(10)\nprint(top_ten)\n",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"
     ]
    }
   ],
   "source": [
    "#Count freq and display top ten\n",
    "count = Counter(words_no_stop)\n",
    "\n",
    "#Top 10 common words\n",
    "top_ten = count.most_common(10)\n",
    "print(top_ten)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
